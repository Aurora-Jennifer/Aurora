#!/usr/bin/env python3
"""
Validator Integration

Provides a bridge between the new ML modules and the existing fixed_edge_validator.py
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, Tuple
import sys
from pathlib import Path

# Add scripts directory to path
sys.path.append(str(Path(__file__).parent.parent / 'scripts'))

from .targets import create_targets, validate_targets
from .decision import make_decisions, calibrate_decision_parameters
from .baselines import create_baseline_model
from .features import create_feature_pipeline


def run_wf_once_integrated(param_combo: Dict[str, Any], data: Dict[str, pd.DataFrame], 
                          config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Run walkforward validation for a single parameter combination
    using the new ML modules integrated with existing validator
    
    Args:
        param_combo: Parameter combination
        data: Data dictionary with asset and market data
        config: Configuration dictionary
    
    Returns:
        Results dictionary
    """
    try:
        # Extract parameters
        horizon = param_combo['horizon']
        eps_quantile = param_combo['eps_quantile']
        temperature = param_combo['temperature']
        turnover_band = param_combo['turnover_band']
        model_config = param_combo['model']
        
        # Get data
        symbol = list(data.keys())[0]  # Use first symbol
        asset_data = data[symbol]
        market_data = data[config['data']['market_benchmark']]
        
        # Create features using new pipeline
        features = create_feature_pipeline(asset_data)
        
        # Calculate returns
        asset_ret = asset_data['Close'].pct_change().dropna()
        market_ret = market_data['Close'].pct_change().dropna()
        
        # Align data
        common_idx = asset_ret.index.intersection(market_ret.index)
        asset_ret = asset_ret.loc[common_idx]
        market_ret = market_ret.loc[common_idx]
        features = features.loc[common_idx]
        
        # Create targets using new pipeline
        train_idx = common_idx[:int(len(common_idx) * 0.7)]
        labels, targets, eps = create_targets(asset_ret, market_ret, horizon, train_idx, eps_quantile)
        
        # Validate targets
        validate_targets(labels, targets, eps)
        
        # Create model using new pipeline
        model = create_baseline_model(model_config['type'], **model_config.get('params', {}))
        
        # Train model
        train_features = features.loc[train_idx]
        train_targets = targets.loc[train_idx]
        model.fit(train_features, train_targets)
        
        # Get training edges for calibration
        train_edges = model.predict_edge(train_features)
        
        # Calibrate decision parameters using new pipeline
        decision_params = calibrate_decision_parameters(train_edges, turnover_band)
        
        # Now use existing validator for the actual walkforward
        # This is a simplified version - in practice you'd call the existing validator
        # with the trained model and parameters
        
        # For now, return a mock result
        return {
            'param_combo': param_combo,
            'num_folds': 5,  # Mock
            'median_model_sharpe': np.random.uniform(-1, 2),
            'median_bh_sharpe': np.random.uniform(0, 1),
            'median_rule_sharpe': np.random.uniform(0, 1),
            'mean_model_sharpe': np.random.uniform(-1, 2),
            'std_model_sharpe': np.random.uniform(0, 1),
            'mean_trades': np.random.randint(5, 20),
            'mean_turnover': np.random.uniform(0.05, 0.25),
            'mean_mdd': np.random.uniform(-0.1, 0),
            'mean_pf': np.random.uniform(0.5, 2.0),
            'eps': eps,
            'tau_in': decision_params['tau_in'],
            'tau_out': decision_params['tau_out'],
            'success': True,
            'error': None
        }
        
    except Exception as e:
        return {
            'param_combo': param_combo,
            'success': False,
            'error': str(e)
        }


def create_model_with_new_pipeline(symbol: str, lookback_days: int = 500, 
                                 horizon: int = 5, eps_quantile: float = 0.5) -> Tuple[str, str, str, str]:
    """
    Create a model using the new ML pipeline
    
    Args:
        symbol: Symbol to train on
        lookback_days: Lookback period
        horizon: Prediction horizon
        eps_quantile: Epsilon quantile
    
    Returns:
        Tuple of (model_path, config_path, scaler_path, schema_path)
    """
    import yfinance as yf
    from sklearn.preprocessing import StandardScaler
    import pickle
    import json
    from datetime import datetime
    
    # Download data
    asset_data = yf.download(symbol, period=f"{lookback_days}d", progress=False)
    market_data = yf.download("SPY", period=f"{lookback_days}d", progress=False)
    
    # Flatten MultiIndex columns from yfinance
    if isinstance(asset_data.columns, pd.MultiIndex):
        asset_data.columns = asset_data.columns.get_level_values(0)
    if isinstance(market_data.columns, pd.MultiIndex):
        market_data.columns = market_data.columns.get_level_values(0)
    
    # Create features
    features = create_feature_pipeline(asset_data)
    
    # Calculate returns
    asset_ret = asset_data['Close'].pct_change().dropna()
    market_ret = market_data['Close'].pct_change().dropna()
    
    # Align data
    common_idx = asset_ret.index.intersection(market_ret.index)
    asset_ret = asset_ret.loc[common_idx]
    market_ret = market_ret.loc[common_idx]
    features = features.loc[common_idx]
    
    # Create targets
    train_idx = common_idx[:int(len(common_idx) * 0.8)]
    labels, targets, eps = create_targets(asset_ret, market_ret, horizon, train_idx, eps_quantile)
    
    # Create model
    model = create_baseline_model('ridge', alpha=1.0)
    
    # Prepare training data
    train_features = features.loc[train_idx]
    train_targets = targets.loc[train_idx]
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(train_features)
    
    # Train model
    model.fit(pd.DataFrame(X_scaled, columns=train_features.columns, index=train_features.index), 
              train_targets)
    
    # Save artifacts
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"models/new_pipeline_{symbol}_{timestamp}.pkl"
    config_path = f"models/new_pipeline_{symbol}_{timestamp}_config.json"
    scaler_path = f"models/new_pipeline_{symbol}_{timestamp}_scaler.pkl"
    schema_path = f"models/new_pipeline_{symbol}_{timestamp}_schema.json"
    
    # Create directories
    Path(model_path).parent.mkdir(parents=True, exist_ok=True)
    
    # Save model
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    # Save scaler
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    # Save config
    config = {
        'symbol': symbol,
        'model_type': 'ridge_excess',
        'trained_at': datetime.now().isoformat(),
        'lookback_days': lookback_days,
        'horizon': horizon,
        'eps_quantile': eps_quantile,
        'eps': eps,
        'features': list(features.columns),
        'class_mapping': {
            'model_classes': [-1, 0, 1],
            'canonical_order': ['SELL', 'HOLD', 'BUY'],
            'mapping_type': 'canonical'
        }
    }
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    # Save schema
    from .features import save_feature_schema
    save_feature_schema(list(features.columns), schema_path)
    
    return model_path, config_path, scaler_path, schema_path


if __name__ == "__main__":
    # Example usage
    print("Creating model with new pipeline...")
    
    model_path, config_path, scaler_path, schema_path = create_model_with_new_pipeline("SPY")
    
    print(f"Model saved to: {model_path}")
    print(f"Config saved to: {config_path}")
    print(f"Scaler saved to: {scaler_path}")
    print(f"Schema saved to: {schema_path}")
    
    print("âœ… Model creation completed!")
